{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f79d3758-8ae6-455c-a4e0-496625a7ee86",
   "metadata": {},
   "source": [
    "# Large Language Models\n",
    "\n",
    "ChatGPT and beyond\n",
    "\n",
    "## Closed and Open models\n",
    "\n",
    "Currently there are several closed-source large language models which are developed by major corparations each took hundreds of millions to train\n",
    "* GPT3.5 (ChatGPT) and GPT4 *by OpenAI*\n",
    "* PaLM and Bard *by Google*\n",
    "* Claude *by Anthropics*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbaa73f-8208-4b9a-a5ba-fca22db0ce91",
   "metadata": {},
   "source": [
    "# Access to OpenAI models\n",
    "\n",
    "There are large language models which are proprietary. You have free access to ChatGPT. There's paid access to GPT4 over OpenAI website.\n",
    "\n",
    "You can also access GPT4 and DALL-E 3 through [Microsoft CoPilot Android App](https://play.google.com/store/apps/details?id=com.microsoft.copilot). The app allows text and image input and output.\n",
    "\n",
    "![app1](images/copilot-app1.jpeg)\n",
    "\n",
    "![app2](images/copilot-app2.jpeg)\n",
    "\n",
    "![app3](images/copilot-app3.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859db315-bd07-43b0-8bb1-30cb336c1b45",
   "metadata": {},
   "source": [
    "\n",
    "# Open source models\n",
    "\n",
    "You can not run ChatGPT or GPT4 locally on your computer (you can have API access though).\n",
    "\n",
    "* OpenAI does not share the model weights\n",
    "* Even if you have weights, you need an expensive computer to run it.\n",
    "\n",
    "Tech giants kept LLMs behind walls. However, in February of 2023, Meta (formerly Facebook) released LLAMA model. This sparked release of open source models. Initial language models were large but then smaller language models were released. These language models can be run in a laptop, locally!\n",
    "\n",
    "Please check the [open source LLM leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard). As of writing this text, models from [Mistral.ai](https://mistral.ai/) were very promising and [Mixtral model was evaluated as good as ChatGPT](https://mistral.ai/news/mixtral-of-experts/) although it's much smaller.\n",
    "\n",
    "You can test LLAMA, Mistral models at [Perplexity AI](https://labs.perplexity.ai/) site.\n",
    "\n",
    "![](images/perplexity-main-page.png)\n",
    "\n",
    "> About Perplexity AI (https://www.perplexity.ai/): Perplexity AI is a chatbot-style search engine that allows users to ask questions in natural language and receive accurate and comprehensive responses. The platform uses AI technology to gather information from multiple sources on the web, including academic databases, news outlets, YouTube, Reddit, and more. Perplexity AI then provides users with a summary containing source citations, enabling them to verify the information and dive deeper into a particular subject. Perplexity AI is a versatile tool that can assist various professions like researchers, writers, artists, musicians, and programmers in multiple tasks such as answering questions, generating text, writing creative content, and summarizing text.\n",
    "> Advantages of Perplexity AI:\n",
    "> * Provides fast and comprehensive answers to complex questions\n",
    "> * Helps users learn new things and explore different perspectives\n",
    "> * Improves usersâ€™ critical thinking and research skills by showing them the sources of the information\n",
    "\n",
    "## Running models locally\n",
    "\n",
    "Since developers share the model weights, it's possible to download and run the models locally. There are various sizes of models. 7B (7 billion) models require around 4Gb memory, so you can run them in your laptop.\n",
    "\n",
    "You can run the models by installing PyTorch and some other libraries for Python and then writing some Python code. Or you can install [Ollama](https://ollama.ai/) and then run any compatible model with it.\n",
    "\n",
    "Advantages of running a model locally:\n",
    "1. Less Censorship\n",
    "2. Better Data Privacy\n",
    "3. Offline Usage\n",
    "4. Cost Savings\n",
    "5. Better Customization\n",
    "\n",
    "Disadvantages of running a model locally:\n",
    "1. Resource Intensive\n",
    "2. Slower Responses and Inferior Performance\n",
    "3. Complex Setup\n",
    "\n",
    "\n",
    "### Running a model locally using Jupyter notebook or Google Colab\n",
    "\n",
    "You can actually run LLM models in Jupyter notebooks. However the process will require you to install lots of Python packages, downloading the model weights and then writing some Python code to ask questions to the model and then capturing the answer in a dictionary. Although there are some solutions for \"chat-like\" experience, the experince is more like \"one-shot question and answer\".\n",
    "\n",
    "Please check the [Youtube video](https://www.youtube.com/watch?v=eovBbABk3hw) which describes the process in Google Colab, where Google provides GPU (or TPU) to run the model in a Jupyter notebook environment.\n",
    "\n",
    "### Running a model locally with Ollama\n",
    "\n",
    "![](images/ollama-main-page.png)\n",
    "\n",
    "Please visit the [list of models page](https://ollama.ai/library) to have an idea about specialized models. With Ollama, you can download and use any of those models.\n",
    "\n",
    "In the terminal, let's list available models\n",
    "\n",
    "```\n",
    "$ ollama list\n",
    "\n",
    "NAME               \tID          \tSIZE  \tMODIFIED\n",
    "deepseek-coder:6.7b\t72be2442d736\t3.8 GB\t5 weeks ago \t\n",
    "neural-chat:latest \t73940af9fe02\t4.1 GB\t5 weeks ago \t\n",
    "orca2:7b           \tea98cc422de3\t3.8 GB\t5 weeks ago \t\n",
    "phi:latest         \tc651b7a89d73\t1.6 GB\t14 hours ago\t\n",
    "solar:latest       \t059fdabbe6e6\t6.1 GB\t2 hours ago \t\n",
    "stablelm-zephyr:3b \t7c596e78b1fc\t1.6 GB\t3 weeks ago\n",
    "```\n",
    "\n",
    "Let's run [Phi-2 by Microsoft](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/). Here's info about Phi-2:\n",
    "\n",
    "> a 2.7 billion-parameter language model that demonstrates outstanding reasoning and language understanding capabilities, showcasing state-of-the-art performance among base language models with less than 13 billion parameters. On complex benchmarks Phi-2 matches or outperforms models up to 25x larger, thanks to new innovations in model scaling and training data curation.\n",
    "\n",
    "```\n",
    "$ ollama run phi:latest\n",
    "\n",
    ">>> why sky is blue?\n",
    "\n",
    " The sky appears blue due to a phenomenon called Rayleigh scattering. When sunlight enters the Earth's atmosphere,\n",
    "it collides with molecules and tiny particles in the air, such as oxygen and nitrogen atoms. These collisions cause\n",
    "the shorter wavelengths of light (blue) to scatter more than the longer wavelengths (red, orange, yellow, green, and\n",
    "violet). As a result, our eyes perceive the scattered blue light to be dominant, which is why the sky appears blue to us.\n",
    "```\n",
    "As you can see, a small model, which can answer questions, help coding can be run locally.\n",
    "\n",
    "### Running a model locally with user interface\n",
    "\n",
    "#### Ollama Web UI\n",
    "\n",
    "![](images/ollama-webui.png)\n",
    "\n",
    "#### LLM Studio\n",
    "\n",
    "You can install [LM Studio](https://lmstudio.ai/) and then interact with local models with its user interface\n",
    "\n",
    "![](images/lm-studio-main.gif)\n",
    "\n",
    "# Coding with LLMs\n",
    "\n",
    "## Github CoPilot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3827bc72-e43e-4c27-bf29-692e2c86ddc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/ZapdeEJ7xJw?si=3DYWvMa7uudDhUnj\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/ZapdeEJ7xJw?si=3DYWvMa7uudDhUnj\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46640657-c196-4bee-baff-fd2a94c99a9d",
   "metadata": {},
   "source": [
    "## Free Github CoPilot alternative\n",
    "\n",
    "Cody is Sourcegraph's AI coding assistant, and it has a couple of features that no other assistant has to make you a 10x developer. You can check it out here: https://sourcegraph.com/cody"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ababfc-4c6c-4241-a365-24cab472ae73",
   "metadata": {},
   "source": [
    "## Open source local alternatives\n",
    "\n",
    "Please visit https://tabby.tabbyml.com/ for more information\n",
    "\n",
    "![](images/tabby-main-page.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0dc3a4-5833-40e5-854e-be36549b3386",
   "metadata": {},
   "source": [
    "# Prompt engineering\n",
    "\n",
    "Please refer to 12-chatgpt lecture for prompt examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ba5600-bf84-4c46-8226-3419a43cca28",
   "metadata": {},
   "source": [
    "# Fine-tuning\n",
    "\n",
    "Fine-tuning a Language Model (LLM) refers to the process of taking a pre-trained language model and further training it on a specific task or domain with a smaller dataset. Fine-tuning allows you to adapt the pre-trained model to a more specialized task or domain, leveraging the knowledge it gained during the initial training. This process is particularly useful when you have a specific language-related task or a dataset that is related to a specific domain, and you want the model to perform well in that specific context.\n",
    "\n",
    "The steps for fine-tuning typically involve taking the pre-trained model, modifying the last layers or adding task-specific layers, and then training the model on the task-specific dataset. Fine-tuning can lead to better performance on the targeted task, as the model has already learned general language patterns and can build upon that knowledge for the specific task at hand.\n",
    "\n",
    "Fine tuning might require a GPU depending on the LLM you are training. Also, if you have rapidly changing domain knowledge then fine tuning is also needed to be performed regulary, which is expensive and cumbersome.\n",
    "\n",
    "# Retrieval Augmented Generation RAG\n",
    "\n",
    "Chat with your documents\n",
    "\n",
    "![](images/rag-diagram.png)\n",
    "\n",
    "image source: [Tarek AbdelKhalek](https://www.linkedin.com/pulse/chat-your-rag-tarek-abdelkhalek)\n",
    "\n",
    "## AutoLLM \n",
    "\n",
    "AutoLLM is capable of performing RAG on your documents. Please watch the video below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c02aa48-5fb5-4484-abaf-55426db58887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Dh9pIWO1VJw?si=VDTJ3PbBDRAgxucS\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Dh9pIWO1VJw?si=VDTJ3PbBDRAgxucS\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cb5c86-9c2a-4de5-bb5d-55b8f058dd0e",
   "metadata": {},
   "source": [
    "# Agents\n",
    "\n",
    "AI agents are frameworks or entities that are designed to perform tasks autonomously using Large Language Models. These agents are capable of perceiving their environment, making decisions, and taking actions to achieve specific goals. The term \"agent\" emphasizes the idea that these entities can act on behalf of a user or a system, often with a degree of autonomy.\n",
    "\n",
    "* [chatdev](https://chatdev.toscl.com): Personalize your AI town\n",
    "* [crew.ai](https://www.crewai.io/): Framework for orchestrating role-playing, autonomous AI agents\n",
    "* [Microsoft Autogen](https://microsoft.github.io/autogen/): Enable next-gen large language model applications\n",
    "\n",
    "In **extras** folder, there are examples for Autogen and CrewAI agent frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5268ec-e528-4ea9-aeac-bff05ecd36d6",
   "metadata": {},
   "source": [
    "# Modes other than text\n",
    "\n",
    "Gemini from Google and GPT4 from OpenAI are multimodal. They support vision and text. And expected to support sound, video etc. soon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b278c11-8755-4884-91be-f86123a86cd5",
   "metadata": {},
   "source": [
    "## Image generation\n",
    "\n",
    "### DALL-E 3 by OpenAI\n",
    "\n",
    "Prompt: \"a dna strand with istanbul city as silhoutte in background\"\n",
    "\n",
    "Revised prompt: \"Imagine a detailed science meets architecture illustration. In the foreground, there's a strand of DNA, magnified and glowing against a dark backdrop. This helix structure is precisely depicted, its familiar double-helix shape twisting majestically. Intertwining with this scientific image, in the background, there's a silhouette representation of Istanbul city. The skyline shows off its marvelous historical architecture, including its domed mosques and towering minarets. The cityscape sits against a dusky eveningsky, the setting sun casting an orange glow on the horizon behind the buildings. The combination results in a fascinating juxtapositionof science and urban beauty.\"\n",
    "\n",
    "Result:\n",
    "\n",
    "![](images/dna-istanbul-dalle.png)\n",
    "\n",
    "### Midjourney v6\n",
    "\n",
    "Midjourney [announced v6](https://mid-journey.ai/midjourney-v6-release/) in December 2023. It can generate photorealistic images. Normally, you need to use their Discord channel to generate images free. Also, you can generate images using [getimg.ai](https://getimg.ai). I tried generating images using the following prompts\n",
    "\n",
    "* Molecular biology lab with lots of students.\n",
    "* Beautiful sea side with white sand beach and tropical trees on shore. The sea looks greenish blue\n",
    "\n",
    "The results are great:\n",
    "\n",
    "![](images/getimg-ai.png)\n",
    "\n",
    "### Disinformation\n",
    "\n",
    "Be aware of deepfake or AI-generated fake photos\n",
    "\n",
    "Fake Trump Photo\n",
    "\n",
    "![](https://media.rnztools.nz/rnz/image/upload/s--SXTQegLG--/ar_16:10,c_fill,f_auto,g_auto,q_auto,w_1050/v1679710393/4LBL8A5_Fake_Trump_3_jfif)\n",
    "\n",
    "Fake Pope Photo\n",
    "\n",
    "![](https://duet-cdn.vox-cdn.com/thumbor/0x0:1281x800/640x427/filters:focal(641x400:642x401):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24539414/ai_pope_drip_god.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c9190f-36ca-4677-a572-fc91779a51bb",
   "metadata": {},
   "source": [
    "## Video generation\n",
    "\n",
    "### Prompt to video\n",
    "* [RunwayML Gen-2](https://runwayml.com/ai-magic-tools/gen-2/)\n",
    "* [Pika](https://pika.art)\n",
    "* [Stable Video Diffusion](https://stability.ai/news/stable-video-diffusion-open-ai-video-model) by Stable Diffusion\n",
    "* [GENMO](https://www.genmo.ai/)\n",
    "\n",
    "### Image to video\n",
    "* [GENMO](https://www.genmo.ai/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7f19de-2076-4eb3-a3b6-e6b82b12548e",
   "metadata": {},
   "source": [
    "## Music or sound generation\n",
    "\n",
    "* [Elevenlabs](https://elevenlabs.io/) can convert text to speech online for free with our AI voice generator\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
